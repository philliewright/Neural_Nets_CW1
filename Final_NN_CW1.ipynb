{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPHrgN4ankyTV/ZqLL3PCkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philliewright/toxic_comments_neural_nets/blob/main/Final_NN_CW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Networks Coursework 1\n",
        "\n",
        "## Toxic Comment Classification\n",
        "\n",
        "## Introduction\n",
        "\n",
        "\n",
        "In the modern, internet driven world, the threat landscape seen in online comments has grown increasingly worrysome and harmful. Polarisation in thinking is increasing and online harrassment for peoples opinions, beliefs and facets of their identity is a huge problem. These comments which can be harmful, offensive and disruptive can have sever impacts on the individuals and communities. We must tackle this problem in order to preserve the integrity and inclusivity of online spaces, in the form of content moderation.\n",
        "\n",
        "We are looking to help takle this problem by using Neural Networks to find and classify toxic comments of different types. Neural Netwroks have the ability to process and learn from huge datasets of text and will be a promising place to come at a solution from. We will be using multilabel classification to attempt to label the comments as either Toxic, Severely Toxic, Obscene, Threat, Insult and Identity hate. We hope this model will be a good start for a tool to maintain healthy online interactions.\n",
        "\n",
        "The origional source of the data is a collection of comments from Wikipedia's talk page edits. The dataset is the Toxic Comment Classification dataset from Kaggle, obtained using the following link:\n",
        "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
        "Given the extensive nature of the dataset we feel it is a realistic cross section of the challenges which we'd expect to encouner in moderating real time online discussions.\n",
        "\n",
        "We are going to follow the methodical approach of Francois Chollet's universal workflow of Deep Learning which we detail below.[1] We want to build a functional model but also fully understand the details of how the neural networks work in the context of NLP.\n",
        "\n",
        "## Method\n",
        "\n",
        "We will be using Chollets universal workflow of Deep Learning as defined in 'Deep Learning with Python' which is a structured approach to developing deep learning models.\n",
        "\n",
        "A summary of this workflow is given below:\n",
        "### 1. Define the Problem and Assemble a Dataset\n",
        "- **Understand the Problem**: We will outline what we are trying to achieve. This includes understanding the problem's nature (classification, regression, etc.) and its complexity. For this project, we are making a classification model.\n",
        "- **Collect a Dataset**: This step is to identify and gather the data we need and amke sure it's representative of the problem we are trying to solve.\n",
        "\n",
        "### 2. Choose a Measure of Success\n",
        "- **Select an Evaluation Protocol and Loss Function**: We have decided to use accuracy, precision, recall and F1 score. We will use the binary cross entropy a loss function, which our model will seek to minimize.\n",
        "\n",
        "### 3. Determine the Evaluation Protocol\n",
        "- **Choose How to Evaluate the Model**: We will use k-fold cross-validation to further evaluate our model.\n",
        "\n",
        "### 4. Prepare Your Data\n",
        "- **Data Preprocessing**: Format the data (normalization, vectorization, etc.) so that it can be fed into a neural network. This step includes splitting the data into training, validation, and test sets.\n",
        "\n",
        "### 5. Develop a Baseline Model\n",
        "- **Build a Small Model**: Start with a simple model (logistic regression) that can process the data and provide a baseline performance. This will mean we can have a point of comparison for more complex models later. We have decided to use a simple Logistic regression as our baseline model.\n",
        "\n",
        "### 6. Scale Up: Develop a Model That Overfits\n",
        "- **Build a Larger Model**: Once we have a baseline, we will build a more complex model capable of overfitting. This step is so that we push the limits to understand the capacity needed to solve the problem.\n",
        "\n",
        "### 7. Regularize and Tune Your Model\n",
        "- **Fight Overfitting**: Use techniques like dropout, regularisation, or feature reduction to mitigate overfitting.\n",
        "- **Hyperparameter Tuning**: Adjust learning rates, batch sizes, number of layers/neurons, etc., to find the best performing model configuration.\n",
        "\n",
        "### 8. Iterate on Your Model\n",
        "- **Refine the Model**: Based on the performance of the model, we will go back and make adjustments. This might include gathering more data, adjusting hyperparameters, or changing the architecture.\n",
        "\n",
        "### 9. Evaluate the Final Model\n",
        "- **Test the Model**: Once the model performs well on the validation data, evaluate it on the test data to estimate how it will perform in the real world.\n"
      ],
      "metadata": {
        "id": "u2UR6TDOydOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary instillations\n",
        "! pip install tensorflow\n",
        "!pip install scikeras\n",
        "!pip install tensorflow-addons\n",
        "!pip install --upgrade tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh4bGYEGhs6n",
        "outputId": "cece3013-d204-4e95-c1e0-03a0443f17a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "E6NAqGVfEoA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.metrics as tf_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from scipy.sparse import vstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "\n",
        "# Download necessary NLTK packages\n",
        "download('punkt')\n",
        "download('stopwords')\n",
        "download('wordnet')\n"
      ],
      "metadata": {
        "id": "RXGvfoCrf6Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define the Problem and Assemble a Dataset\n",
        "\n",
        "The problem we are trying to solve is - what configuration of a Neural Network can best be used to classify toxic comments online in order to better understand the threat landscape and potentially contribute to mitigating these threats. To do this we will be using classification techniques and Dense and Dropout layers in our Neural Network."
      ],
      "metadata": {
        "id": "EZDIHurk_xjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the GDrive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to folder in GDrive\n",
        "folder_path = '/content/drive/MyDrive/Projects_Portfolio/toxic_comment_neural_nets'\n",
        "\n"
      ],
      "metadata": {
        "id": "WvD2KYej_w8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg9kJnC8yX6s"
      },
      "outputs": [],
      "source": [
        "# listing all files in the folder to check its correct\n",
        "all_files = os.listdir(folder_path)\n",
        "all_files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# constructing the file paths\n",
        "train_file_path = os.path.join(folder_path, 'train.csv')\n",
        "test_file_path = os.path.join(folder_path, 'test.csv')\n",
        "test_labels_file_path = os.path.join(folder_path, 'test_labels.csv')\n",
        "\n",
        "\n",
        "# Reading in the CSV files\n",
        "train_data = pd.read_csv(train_file_path)\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "test_labels = pd.read_csv(test_labels_file_path)\n"
      ],
      "metadata": {
        "id": "UuvnUv1MAdFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data.head()\n",
        "#test_data.head()\n",
        "test_labels.head()\n",
        "\n",
        "# Removing rows where any label is -1 as there was some contamination in the data, but only a small amount\n",
        "valid_test_labels = test_labels[(test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] != -1).all(axis=1)]\n",
        "y_test_filtered = valid_test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "\n",
        "\n",
        "#print(\"\\nUnique Values for 'column_name':\\n\", unique_values)\n",
        "y_test_filtered.head()"
      ],
      "metadata": {
        "id": "bLSNG9CFAjqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'id' column to y_test_filtered for when we merge it\n",
        "y_test_filtered['id'] = valid_test_labels['id']\n",
        "\n",
        "# Merge test_data with y_test_filtered based on 'id'\n",
        "merged_test_data = pd.merge(test_data, y_test_filtered, on='id')\n",
        "\n",
        "# making new aligned X_test and y_test\n",
        "X_test_aligned = merged_test_data['comment_text']\n",
        "y_test_aligned = merged_test_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n"
      ],
      "metadata": {
        "id": "xKugL1vbDZ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting training and test data\n",
        "X_train = train_data['comment_text']\n",
        "y_train = train_data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "X_test = X_test_aligned\n",
        "y_test = y_test_aligned[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
      ],
      "metadata": {
        "id": "kqhqX1qDDjWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of rows in X_train and X_test\n",
        "num_rows_X_train = X_train.shape[0]\n",
        "num_rows_X_test = X_test.shape[0]\n",
        "\n",
        "# Print the counts\n",
        "print(\"Number of rows in X_train:\", num_rows_X_train)\n",
        "print(\"Number of rows in X_test:\", num_rows_X_test)\n"
      ],
      "metadata": {
        "id": "FRweL5n3DqBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descriptive stats of the dataset:\n",
        "\n",
        "# distribution of different toxicity labels\n",
        "label_counts = y_train.sum()\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=label_counts.index, y=label_counts.values, alpha=0.8)\n",
        "plt.title('Frequency of Different Labels in the Training Data')\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Toxicity Label', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# length of comments in the training and test sets\n",
        "train_comment_lengths = X_train.apply(lambda x: len(x.split()))\n",
        "test_comment_lengths = X_test.apply(lambda x: len(x.split()))\n",
        "\n",
        "# comment distributiion by lenght\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.hist(train_comment_lengths, bins=60, alpha=0.7, label='Train Comments')\n",
        "plt.hist(test_comment_lengths, bins=60, alpha=0.7, label='Test Comments')\n",
        "plt.title('Comment Length Distribution in Train and Test Data')\n",
        "plt.xlabel('Comment Length (Number of Words)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# heatmap of the labels correlation\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(y_train.corr(), annot=True, fmt=\".2f\")\n",
        "plt.title('Correlation between Different Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ugLNBzDfpgjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that 'toxic' has by far the most number of comments labelled. We can also see there is a huge amount of imbalence so try to correct. The distribution of lengths of comments seem similar for Test and Train data which is good as it suggests the model will be trained on representative data in terms of comment length.\n",
        "\n",
        "The heatmap shows a very high correlation between 'obscene' and 'insult' comments as well as 'insult' and 'toxic' which means many comments that are labelled one are also labelled the other. It is interesting that 'threat' has low corellations with all of the other labels, suggesting not much of a crossover. This could be because there are unique charachteristics of 'threat' comments which may require special consideration in the modelling"
      ],
      "metadata": {
        "id": "rOnMhwT4qpv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Choosing a measure of success\n",
        "\n",
        "For this model we will be evaluating accuracy, precision, recall and F1 score. In our case accuracy may not be the most informative metric as it can often be skewed to the majority class. F1 is especially cruicial for imbalenced datasets as it measures the balence between precision and recall. In our case, missing a toxic comment and having lower recall may be more problematic than classifying non toxic comments as toxic.\n",
        "\n",
        "We will also be using the binary cross entropy a loss function, which our model will try to minimise. Binary cross entropy will calculate the loss for each of the classes and aggregate these to penalise the incorrect classifications.\n",
        "\n",
        "We will also experiment with including the Receiver Operating Characteristic - Area Under Curve (ROC-AUC) score. This metric is particularly good in imbalanced datasets as it evaluates the model's ability to distinguish between classes across different threasholds, providing a measure of performance unaffected by class imbalance.\n",
        "\n",
        "Given the imbalances in our dataset, we will explore incorporating class weights in our loss function. This approach will help in adjusting the model's sensitivity to underrepresented classes so that we have a balanced performance across all categories of toxicity.\n",
        "\n",
        "We hope that measuring all of these as they will give a well rounded idea of how the model is performing. Binary cross entropy is a good loss function when dealing with multilabel classification as it calculates the loss for each class and then sums them up. This allows the model to penalise incorrect classifications effectively.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wbxv1OvxDr1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Determining how to Evaluate the Model\n",
        "\n",
        "Once we have a model which seems to perform well, we will impletment k-fold cross-validation to evaluate our model. It will help in assessing how the model's performance will generalise to an independent dataset and mitigates the risk of overfitting to the training data. We will then of course test it on unseen test data as a final evaluation of performance comparing two models for their performance on the unseen test data."
      ],
      "metadata": {
        "id": "kZEg3oBaQ4w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Adding MLFLow\n"
      ],
      "metadata": {
        "id": "6H8_ARfvFix9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"Toxic Comment Classification\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OBsTiXtFFhS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preparing the data\n",
        "\n",
        "To effectively prepare the data for training we will complete the following steps:\n",
        "\n",
        "\n",
        "1.   Remove Stop words from the text - this reduces the size of the data for efficiency as well as taking away words which are likely to not contribute to context.\n",
        "2.   Tokenise - this splits the text into words (tokens). This is an important step in getting unstructred text into a structured form.\n",
        "3.   Lemmatise - This is grouping together different inflected forms of the same words and reducing them to their root form.It helps to consolidate different forms of a word into a single entity.\n",
        "4. Vectorisation - This is needed to convert the text into numerical vectors. We will be using TF-IDF  (Term Frequency-Inverse Document Frequency)  vectorisation using 500 features. There is a risk that only using 500 features will miss some nuances of that data. However it will be more efficient and reduce computational load.\n",
        "\n"
      ],
      "metadata": {
        "id": "2nAQa9rGS8di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Stopwords and Tokenising"
      ],
      "metadata": {
        "id": "6judE881Vfef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seting English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# tokenise the comments and remove stopwords\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# applying the tokenisation and stopword removal to training and test data\n",
        "X_train_tokens = X_train.apply(tokenize_and_remove_stopwords)\n",
        "X_test_tokens = X_test.apply(tokenize_and_remove_stopwords)\n"
      ],
      "metadata": {
        "id": "SNiRR91iU38f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatising"
      ],
      "metadata": {
        "id": "TpewjId4VjA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatise\n",
        "def lemmatize_tokens(tokens):\n",
        "    return ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n",
        "# Apply lemmatization to tokenized data\n",
        "X_train_preprocessed = X_train_tokens.apply(lemmatize_tokens)\n",
        "X_test_preprocessed = X_test_tokens.apply(lemmatize_tokens)\n"
      ],
      "metadata": {
        "id": "0y91vlWCVbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorising\n",
        "\n",
        "For our vectorising we will be using TF-IDF Vectorisation. TF-IDF stands for Term Frequency-Inverse Document Frequency which is a measurement used to show how important a word is in any document or collection, here the comments. It is used in vectorisation to transform data into numerical vecors so that a machine lerning algorithm can learn, predict and classify from it. As most terms will not appear in all comments, the resulting matrix will be very sparse. We will not convert the matrix into a dense array due to computational resource constraints and this is a highly memory intensive process. When needed we can convert the matrix in batches for training.\n"
      ],
      "metadata": {
        "id": "VhHhfs28Vl5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization (Keepinf as sparse matrix, will make desnse in generator)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_train_tfidf_sparse = tfidf_vectorizer.fit_transform(X_train_preprocessed)\n",
        "X_test_tfidf_sparse = tfidf_vectorizer.transform(X_test_preprocessed)#"
      ],
      "metadata": {
        "id": "DM7piGJ7VkwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conducting some exploratory analyis to ensure the vectorisation is succesful and looking at the distribution of labels"
      ],
      "metadata": {
        "id": "Zde7ZBTbX1Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many comments\n",
        "num_samples = X_train_tfidf_sparse.shape[0]\n",
        "print(f\"Number of samples in training data: {num_samples}\")\n",
        "\n",
        "# cgeck number of features after vectorising\n",
        "num_features = X_train_tfidf_sparse.shape[1]\n",
        "print(f\"Number of features (after TF-IDF): {num_features}\")\n",
        "\n",
        "# checking the sparsity of the matrix - expecting very sparse\n",
        "non_zero_elements = X_train_tfidf_sparse.nnz\n",
        "total_elements = X_train_tfidf_sparse.shape[0] * X_train_tfidf_sparse.shape[1]\n",
        "sparsity = (1.0 - non_zero_elements / float(total_elements))\n",
        "\n",
        "print(f\"Sparsity of the TF-IDF matrix: {sparsity:.2%}\")\n",
        "\n",
        "# distribution of Labels\n",
        "label_counts = y_train.sum().sort_values(ascending=False)\n",
        "print(\"Label distribution:\\n\", label_counts)\n",
        "\n"
      ],
      "metadata": {
        "id": "1U0HPW-qXxbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above results show that the dataset has 159571 comments to train the data from. The TF-IDF has vectorised using 500 features as we specified in the vectorisation to limit the TF-IDF to ensure the dataset was not too large for efficiency. Therefore only the top 500 most common words will be analysed.\n",
        "\n",
        "The sparsity of 97.15% is expected as most words wont appear in all the comments.\n",
        "\n",
        "The distribution in the labels shows threat to be the least common label and toxic to be the most. There is a class imbalence here which is a challenge we will address within out Neural Network."
      ],
      "metadata": {
        "id": "jJfmQtToYg_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Develop a Baseline Model\n",
        "\n",
        "In order to evaluate how well the model the models perform and ensure the data has been processed effectively, it is good to establish a baseline model. We will then use this to compare future models performance.\n",
        "\n",
        "We have decided to use a simple neural netowrk, with no dropout layers or complext forms of regularisation. This is to see how well the model performs without these complexities.\n",
        "\n",
        "We also used a Sparse Data generator function. This was to efficiently feed batches of the sparse data to the network. This is because the data would otherwise not fit into memory all at once as it is very large.\n",
        "\n",
        "source: https://dzlab.github.io/dltips/en/keras/data-generator/\n"
      ],
      "metadata": {
        "id": "kqOFyEiYZPZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Matrix shape: {X_train_tfidf_sparse.shape}\")\n",
        "print(f\"Non-zero elements: {X_train_tfidf_sparse.nnz}\")\n"
      ],
      "metadata": {
        "id": "Hccxm57Vm_FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SparseDataGenerator class\n",
        "class SparseDataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.ceil(self.x.shape[0] / self.batch_size).astype(int)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "      batch_y = self.y.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "      # Debugging print statements\n",
        "\n",
        "\n",
        "      return batch_x.toarray(), np.array(batch_y)\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_tfidf_sparse, y_train, test_size=0.1, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "dADkjH-BofDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # starting an MLflow run\n",
        "    mlflow.start_run(run_name=\"Baseline Model\")\n",
        "\n",
        "    batch_size = 32\n",
        "\n",
        "    # data generators\n",
        "    train_gen = SparseDataGenerator(X_train_split, y_train_split, batch_size)\n",
        "    val_gen = SparseDataGenerator(X_val_split, y_val_split, batch_size)\n",
        "\n",
        "    # parameters\n",
        "    params = {\n",
        "        \"num_layers\": 2,\n",
        "        \"num_neurons_layer1\": 128,\n",
        "        \"num_neurons_output\": 6,\n",
        "        \"activation_layer1\": \"relu\",\n",
        "        \"activation_output\": \"sigmoid\",\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"loss_function\": \"binary_crossentropy\",\n",
        "        \"batch_size\": 32\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # defining the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_dim=X_train_tfidf_sparse.shape[1]))\n",
        "    model.add(Dense(6, activation='sigmoid'))  # 6 output neurons for 6 labels\n",
        "\n",
        "    # compiling\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[\n",
        "        'accuracy', tf_metrics.Precision(name='precision'), tf_metrics.Recall(name='recall'),\n",
        "        tfa.metrics.F1Score(num_classes=6, average='macro', name='f1_score')\n",
        "    ])\n",
        "\n",
        "    # training\n",
        "    history = model.fit(train_gen, epochs=10, validation_data=val_gen)\n",
        "\n",
        "    # logging metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # adding model to the log\n",
        "    mlflow.keras.log_model(model, \"baseline_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "biQVgWk8nAah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Training and Validation Accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting Training and Validation Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, and F1 Score\n",
        "metrics = ['precision', 'recall', 'f1_score']\n",
        "for m in metrics:\n",
        "    plt.plot(history.history[m])\n",
        "    plt.plot(history.history[f'val_{m}'])\n",
        "    plt.title(f'Model {m.capitalize()}')\n",
        "    plt.ylabel(m.capitalize())\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ef6ANeXLpOaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Baseline Model\n",
        "\n",
        "- **Loss** - the training loss decreased over the epochs which indicates the model is learning and improving its predictions. The validation loss also decreased but slightly increased in later epochs which could be the result of overfitting.\n",
        "\n",
        "- **Accuracy** - There is a decrease in accuracy over the epochs which is odd. We would normally expect accuracy to rise over the epochs. The validation accuracy fluctuates which could be due to class imbalence or the model not generalising well.\n",
        "\n",
        "- **Precision and Recall** - The precision and recall slightly improve over the epochs.\n",
        "\n",
        "- **F1 Score** - This is a balence between precision and recall which slighly improves over the epochs in the training data but decreased in the validation data. It may be due to the class imbalence or the complexity.\n",
        "\n",
        "The decrease in accuracy as well as an increase in precision and recall suggests that the model might be becoming more conservative in predicting the positive class (toxic comments). This actually could be good for a toxic comment classifier as false positives (non-toxic comments classified as toxic) are usually more harmful than false negatives. The F1 score, a balance between precision and recall, is also improving, indicating a better balance between the two.\n",
        "\n",
        "Moving forward we will look to improve the model by experimenting with drop out layers, class imbalences and other model archetecture & hyperparameters."
      ],
      "metadata": {
        "id": "Ap7wXOnmpi6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Develop a model which overfits\n",
        "\n",
        "For this step we will develop a more complex model which is capable of overfirring. This will include adding more layers and more neurons in each layer. This will help us to understand the capacity that we will need for this model."
      ],
      "metadata": {
        "id": "mM71wzvJrHYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # MLflow run for the complex model\n",
        "    mlflow.start_run(run_name=\"Complex Model\")\n",
        "\n",
        "    # Data generators -- keeping the same\n",
        "    batch_size = 32\n",
        "    train_gen = SparseDataGenerator(X_train_split, y_train_split, batch_size)\n",
        "    val_gen = SparseDataGenerator(X_val_split, y_val_split, batch_size)\n",
        "\n",
        "    # parameters for the complex model\n",
        "    complex_params = {\n",
        "        \"num_layers\": 4,\n",
        "        \"num_neurons_layer1\": 512,\n",
        "        \"num_neurons_layer2\": 256,\n",
        "        \"num_neurons_layer3\": 128,\n",
        "        \"num_neurons_layer4\": 64,\n",
        "        \"num_neurons_output\": 6,\n",
        "        \"activation_layer\": \"relu\",\n",
        "        \"activation_output\": \"sigmoid\",\n",
        "        \"dropout_rate\": 0.5,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"loss_function\": \"binary_crossentropy\",\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "    mlflow.log_params(complex_params)\n",
        "\n",
        "    # Define the more complex model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, activation='relu', input_dim=X_train_tfidf_sparse.shape[1]))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[\n",
        "        'accuracy', tf_metrics.Precision(name='precision'), tf_metrics.Recall(name='recall'),\n",
        "        tfa.metrics.F1Score(num_classes=6, average='macro', name='f1_score')\n",
        "    ])\n",
        "\n",
        "    # train the model\n",
        "    history = model.fit(train_gen, epochs=10, validation_data=val_gen)\n",
        "\n",
        "    # log metrics\n",
        "    complex_metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(complex_metrics)\n",
        "\n",
        "    # log model\n",
        "    mlflow.keras.log_model(model, \"complex_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "XKf4KMzpro2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot of training and Validation Accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting Training and Validation Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, and F1 Score\n",
        "metrics = ['precision', 'recall', 'f1_score']\n",
        "for m in metrics:\n",
        "    plt.plot(history.history[m])\n",
        "    plt.plot(history.history[f'val_{m}'])\n",
        "    plt.title(f'Model {m.capitalize()}')\n",
        "    plt.ylabel(m.capitalize())\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "xsgiyT24r-Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the More Complex Model\n",
        "- **Loss**: The training loss steadily decreased from 0.0889 to 0.0628, suggesting effective learning. However, the validation loss shows a slight increase in the later epochs, this could be due to overfitting.\n",
        "\n",
        "- **Accuracy**: The training accuracy started high and remained relatively stable, with a slight decrease to 99.01% by the final epoch. The validation accuracy, however, remained constant at 99% throughout, which is unusually high and stable..\n",
        "\n",
        "- **Precision and Recall**: Both metrics improved over the epochs. The final training precision and recall are 82.66% and 52.36%, respectively. In validation, they are 79.21% and 44.00%. Not a huge improvement though.\n",
        "\n",
        "- **F1 Score**: The F1 score showed minimal improvement in training but remained almost constant in validation. The lack of significant improvement in the F1 score suggests challenges in balancing precision and recall.\n",
        "\n",
        "Conclusion:\n",
        "The model demonstrates some of the characteristics of overfitting, e.g. the consistent validation metrics despite improving training metrics.\n",
        "The constant high validation accuracy might imply issues with the validation set or an imbalance in the dataset.\n",
        "To improve we will look at othe ways of tuning of the architecture, and experimenting with techniques to combat overfitting."
      ],
      "metadata": {
        "id": "QCAAUdP2yA7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Regularise and Tune Model\n",
        "\n",
        "## 7.1 Using Class Weights\n",
        "\n",
        "As the dataset is imbalenced, I will experiemnt with using class weights to see if the metrics are improved. As this is a multi-label problem, not just a multi-class problem, it is not straight forward to do this. Assigning a single class weight does not capture the complexity, so instead we consider the importance of each sample when calculating class weights.\n",
        "\n",
        "https://grabngoinfo.com/imbalanced-multi-label-classification-balanced-weights-may-not-improve-your-model-performance/\n",
        "\n",
        "https://gist.github.com/angeligareta/83d9024c5e72ac9ebc34c9f0b073c64c\n"
      ],
      "metadata": {
        "id": "pocRWQNtUyzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if multiclass or multilabel\n",
        "is_multi_label = (y_train.sum(axis=1) > 1).any()\n",
        "\n",
        "if is_multi_label:\n",
        "    print(\"This is a multi-label dataset.\")\n",
        "else:\n",
        "    print(\"This is a multi-class dataset.\")\n"
      ],
      "metadata": {
        "id": "PpTWhiLQZaaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Start an MLflow run for the model with class weights\n",
        "    mlflow.start_run(run_name=\"Model with Class Weights\")\n",
        "\n",
        "    # data gens\n",
        "    batch_size = 32\n",
        "    train_gen = SparseDataGenerator(X_train_split, y_train_split, batch_size)\n",
        "    val_gen = SparseDataGenerator(X_val_split, y_val_split, batch_size)\n",
        "\n",
        "    # using the same parameters as the complex model\n",
        "    params_with_class_weights = complex_params  # Assuming complex_params is already defined\n",
        "    mlflow.log_params(params_with_class_weights)\n",
        "\n",
        "    # calculate class weights\n",
        "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "    class_weights = np.mean(sample_weights) / sample_weights\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    # log the class weights\n",
        "    mlflow.log_dict(class_weight_dict, \"class_weights.json\")\n",
        "\n",
        "    # Define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, activation='relu', input_dim=X_train_tfidf_sparse.shape[1]))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[\n",
        "        'accuracy', tf_metrics.Precision(name='precision'), tf_metrics.Recall(name='recall'),\n",
        "        tfa.metrics.F1Score(num_classes=6, average='macro', name='f1_score')\n",
        "    ])\n",
        "\n",
        "    # train the model with class weights\n",
        "    history = model.fit(train_gen, epochs=10, validation_data=val_gen, class_weight=class_weight_dict)\n",
        "\n",
        "    # log metrics\n",
        "    metrics_with_class_weights = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics_with_class_weights)\n",
        "\n",
        "    # logging model\n",
        "    mlflow.keras.log_model(model, \"model_with_class_weights\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "ldUzPqxycQPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Training and Validation Accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting Training and Validation Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, and F1 Score\n",
        "metrics = ['precision', 'recall', 'f1_score']\n",
        "for m in metrics:\n",
        "    plt.plot(history.history[m])\n",
        "    plt.plot(history.history[f'val_{m}'])\n",
        "    plt.title(f'Model {m.capitalize()}')\n",
        "    plt.ylabel(m.capitalize())\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "JaS6AYpSVYeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weighted Model Eval:**\n",
        "\n",
        "- **Loss:** The weighted model has extremely high training loss, suggesting issues with convergence/ learning rate. Valloss is low but stable - shows ineffective learning.\n",
        "\n",
        "- **Accuracy:** Training accuracy is high, but this might be misleading as there is the high loss. Validation accuracy is also high, suggesting poor generalisation.\n",
        "\n",
        "- **Precision and Recall:** Precision and recall are relatively stable but not good, with values around 80% and recall values between 40-60%.\n",
        "\n",
        "- **F1 Score:** The F1 score is very low\n",
        "\n",
        "**Comparisons:**\n",
        "\n",
        "- The baseline model shows some improvements in metrics over the epochs but has issues with accuracy and F1 score, could be because of potential problems with class imbalance or model complexity.\n",
        "\n",
        "- The overfitting model performs well on training data but fails to generalise to the validation set, suggesting overfitting maybe.\n",
        "\n",
        "- The weighted model has issues with convergence, training accuracy, and F1 score, indicating that it struggles to find a balance between precision and recall, possibly due to class imbalance or other issues.\n",
        "\n",
        "- The overfitting model's high training accuracy suggests that it may need regularisation techniques to improve generalisation. It is clear that the weighted model requires significant adjustments to address convergence and class imbalance issues.\n",
        "\n",
        "\n",
        "- Maybe because it is multilabel instead of multi class there are more complications than anticipated. The inverse weighting was used to weight based on frequency but that might be what is leading to the extremely high losses as the model is being excessively penalised for misclassification in rare cased.\n",
        "- We will therefore look at some alternatives to deal with the class imablence."
      ],
      "metadata": {
        "id": "BIi6QeNxj2xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# As Class Weights seem to be problematic after trying a few methods, we will try instead to use oversampling methods"
      ],
      "metadata": {
        "id": "Z6l8u1YSzAww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.2 Using Oversampling to Balence the dataset\n",
        "\n",
        "We will try to use a sijmplified oversampler which will only oversample each individual label as opposed to overlapping labels."
      ],
      "metadata": {
        "id": "_evTDxfPk9hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n"
      ],
      "metadata": {
        "id": "ZJm_JXoelCqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will look at the samples with overlapping classes\n",
        "# Count the number of labels per sample\n",
        "labels_per_sample = y_train.sum(axis=1)\n",
        "\n",
        "# Count how many samples have 2 or more labels\n",
        "samples_with_two_or_more_labels = (labels_per_sample >= 2).sum()\n",
        "\n",
        "print(f\"Number of samples with 2 or more labels: {samples_with_two_or_more_labels}\")\n"
      ],
      "metadata": {
        "id": "da6R5wFNlHI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Labels again so can see here\n",
        "label_counts = y_train.sum().sort_values(ascending=False)\n",
        "print(\"Label distribution:\\n\", label_counts)"
      ],
      "metadata": {
        "id": "Ppj1RWV3XbEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find single-label instances\n",
        "single_label_mask = y_train.sum(axis=1) == 1\n",
        "X_train_single_label = X_train_tfidf_sparse[single_label_mask]\n",
        "y_train_single_label = y_train[single_label_mask]\n",
        "\n",
        "# need to convert y_train to a format compatible with RandomOverSampler\n",
        "y_train_single_label_combined = y_train_single_label.idxmax(axis=1)\n",
        "\n",
        "# using RandomOverSampler to oversample single-label instances\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_single_label_resampled, y_train_single_label_combined_resampled = ros.fit_resample(X_train_single_label, y_train_single_label_combined)\n",
        "\n",
        "# transforming y_train_single_label_combined_resampled back to multi-label format\n",
        "y_train_single_label_resampled_df = pd.DataFrame(0, index=np.arange(len(y_train_single_label_combined_resampled)), columns=y_train.columns)\n",
        "for i, label in enumerate(y_train_single_label_combined_resampled):\n",
        "    y_train_single_label_resampled_df.at[i, label] = 1\n",
        "\n",
        "# now we combine the oversampled single-label data with the rest of the data\n",
        "X_train_rest = X_train_tfidf_sparse[~single_label_mask]\n",
        "y_train_rest = y_train[~single_label_mask]\n",
        "\n",
        "X_train_resampled = vstack([X_train_single_label_resampled, X_train_rest])\n",
        "y_train_resampled = pd.concat([y_train_single_label_resampled_df, y_train_rest])\n",
        "\n"
      ],
      "metadata": {
        "id": "ijnY7MCzb2L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Labels\n",
        "label_counts = y_train_resampled.sum().sort_values(ascending=False)\n",
        "print(\"Label distribution of resampled:\\n\", label_counts)"
      ],
      "metadata": {
        "id": "4PcUAygllbLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are still very imbalenced, now trying to use Label Powerset Method\n",
        "This treats each unique set of labels as a single label in a multi-class problem. We can then go back and try  traditional over-sampling methods. This can significantly increase the number of classes and may not be practical if there are many combinations of labels. But i think here it should be oK."
      ],
      "metadata": {
        "id": "HM-o4JYZoGDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ok now trying to convert multi-label problem to a multi-class problem using Label Powerset\n",
        "y_train_powerset = y_train.apply(lambda x: '-'.join([str(i) for i in range(len(x)) if x[i]]), axis=1)\n",
        "\n",
        "# first initialise RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Resample both features and label powerset\n",
        "X_train_resampled_powerset, y_train_resampled_powerset = ros.fit_resample(X_train_tfidf_sparse, y_train_powerset)\n",
        "\n",
        "# Convert the resampled label powerset back to multi-label format\n",
        "y_train_resampled_2 = pd.DataFrame(0, index=np.arange(len(y_train_resampled_powerset)), columns=y_train.columns)\n",
        "for idx, label_combination in enumerate(y_train_resampled_powerset):\n",
        "    if label_combination:  # Check if the string is not empty\n",
        "        labels = label_combination.split('-')\n",
        "        for label in labels:\n",
        "            y_train_resampled_2.at[idx, y_train.columns[int(label)]] = 1\n",
        "\n",
        "# Combine the resampled data\n",
        "X_train_resampled_2 = vstack([X_train_resampled_powerset])\n",
        "y_train_resampled_2 = pd.concat([y_train_resampled_2])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FYkPAlfOoFtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Labels\n",
        "label_counts = y_train_resampled_2.sum().sort_values(ascending=False)\n",
        "print(\"Label distribution of resampled:\\n\", label_counts)"
      ],
      "metadata": {
        "id": "qE4iY3_2wAYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Althoguh not perfect, this is much more balenced than previously. However there are many many more samples. The numbers have dramatically increased because now we are counting each unique combination as a separate class.Labels like 'toxic', which often appear with other labels, will have a high count because they are part of many unique combinations. With more classes to predict the model will require much more computational resource. we will therefore use a 10% subset of the data due to limited resources. We will ensure that it is a representative subset by randomly selecting."
      ],
      "metadata": {
        "id": "fhK2PpB5wmHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "# Calculate 10% of the data length\n",
        "subset_size = int(0.1 * X_train_resampled_2.shape[0])\n",
        "\n",
        "# Generate random indices\n",
        "random_indices = np.random.choice(X_train_resampled_2.shape[0], size=subset_size, replace=False)\n",
        "\n",
        "# Subset the data using these indices\n",
        "# For sparse matrix\n",
        "if isinstance(X_train_resampled_2, csr_matrix):\n",
        "    X_train_resampled_2 = X_train_resampled_2[random_indices, :]\n",
        "# For DataFrame\n",
        "else:\n",
        "    X_train_resampled_2 = X_train_resampled_2.iloc[random_indices]\n",
        "\n",
        "# For y_train_resampled_2, assuming it's a DataFrame\n",
        "y_train_resampled_2 = y_train_resampled_2.iloc[random_indices]\n",
        "\n",
        "# Now, recreate the data generators with the subset\n",
        "train_gen_resampled = SparseDataGenerator(X_train_resampled_2, y_train_resampled_2, batch_size)\n",
        "val_gen_resampled = SparseDataGenerator(X_val_split, y_val_split, batch_size)\n"
      ],
      "metadata": {
        "id": "FBZ_obU7dRSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n"
      ],
      "metadata": {
        "id": "0vTBpiiByo0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Start an MLflow run for the model trained with resampled data\n",
        "    mlflow.start_run(run_name=\"Model with Resampled Data\")\n",
        "\n",
        "    # Assuming 'params' is already defined with the model's parameters\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # Train the model with resampled data\n",
        "    history_resampled = model.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled\n",
        "    )\n",
        "\n",
        "    # Log metrics for the resampled training\n",
        "    metrics_resampled = {\n",
        "        \"train_accuracy\": history_resampled.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history_resampled.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history_resampled.history['precision'][-1],\n",
        "        \"validation_precision\": history_resampled.history['val_precision'][-1],\n",
        "        \"train_recall\": history_resampled.history['recall'][-1],\n",
        "        \"validation_recall\": history_resampled.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history_resampled.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history_resampled.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics_resampled)\n",
        "\n",
        "    # Log the model trained with resampled data\n",
        "    mlflow.keras.log_model(model, \"model_resampled_data\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "JnKnPbjBwl1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Label POwerset Method\n",
        "This method shows a significant increase in precision and recall compared to other class weight method.\n",
        "The F1-score is substantially higher, indicating a better balance between precision and recall. However, the model accuracy is lower, but this is more representative due to the improved balance in other metrics.\n",
        "\n",
        "It seems that the high accuracy in other models may have bene misleading sue to the class imbalence, usually the F1 score is a more reliable indicator with imbalenfed datasets. Therefore we will continue to use this method, now adding som regularisation to see if we can improve accuracy and performance further.\n"
      ],
      "metadata": {
        "id": "-lAOm0mZ9MCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 Regularisation\n",
        "### Understanding L1 and L2 regularisation\n",
        "\n",
        "For training neural networks, regularisation is a crucial technique used to prevent overfitting, ensuring that the model generalises well to unseen data. L1 and L2 regularisation are two commonly used methods. They work by adding a penalty to the loss function, which constrains the size of the model weights.\n",
        "\n",
        "#### L1 regularisation (Lasso regularisation)\n",
        "- **Mechanism**: L1 regularisation adds a penalty equal to the absolute value of the magnitude of the coefficients. Mathematically, it adds the sum of the absolute values of the weights to the loss function.\n",
        "- **Formula**: The L1 penalty is given by |w|, where  is the regularisation parameter and w represents the weights.\n",
        "- **Effect on Weights**: L1 can lead to zeroing out some weights, effectively performing feature selection. This is useful when we suspect that some features might not be relevant or when we want a sparse model. This is the case with our sparse dataset as text data often is.\n",
        "- **Use Cases**: It's particularly good in scenarios where we need a model with fewer parameters or when we're dealing with high-dimensional data where feature selection is useful.\n",
        "\n",
        "#### L2 regularisation (Ridge regularisation)\n",
        "- **Mechanism**: L2 regularisation adds a penalty equal to the square of the magnitude of the coefficients. The penalty term is the sum of the squared weights.\n",
        "- **Formula**: The L2 penalty is given by w, where  is the regularisation parameter and w represents the weights.\n",
        "- **Effect on Weights**: Unlike L1, L2 regularisation doesn't lead to zero coefficients. Instead, it encourages the weights to be small but doesn't force them to zero. This results in a model where the contribution of each feature is samller but included.\n",
        "- **Use Cases**: L2 is widely used in situations where we don't want to exclude any features but want to penalize large weights that might lead to overfitting.\n",
        "\n",
        "\n",
        "In some cases, a combination of both L1 and L2 regularisation (known as Elastic Net regularisation) is used to leverage the benefits of both methods. This approach can be particularly powerful when dealing with complex datasets with a mix of relevant and irrelevant features. For this task we are going to experiment with different values of both L1 ans L2 regularisation"
      ],
      "metadata": {
        "id": "wxC7xYl8U5ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2.1 L2 = 0.001"
      ],
      "metadata": {
        "id": "5V22n8R2p3Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Regularisation L2=0.01\")\n",
        "\n",
        "    # log model parameters\n",
        "    params = {\n",
        "        \"regularisation\": \"l2\",\n",
        "        \"l2_value\": 0.001,\n",
        "        \"input_dim\": X_train_tfidf_sparse.shape[1],\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # model training\n",
        "    history = model.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled\n",
        "    )\n",
        "\n",
        "    # log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # log model\n",
        "    mlflow.keras.log_model(model, \"model_regularisation_l2_0.01\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "    # end the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "iK6F7WirU9Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# edit plots to use new data"
      ],
      "metadata": {
        "id": "4V80PRFvzt1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting training and validation accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting Training and Validation Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Evaluations\n",
        "metrics = ['precision', 'recall', 'f1_score']\n",
        "for m in metrics:\n",
        "    plt.plot(history.history[m])\n",
        "    plt.plot(history.history[f'val_{m}'])\n",
        "    plt.title(f'Model {m.capitalize()}')\n",
        "    plt.ylabel(m.capitalize())\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "OEebG7P6qAIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating resampled data model with L2=0.001\n",
        "\n",
        "**Loss and Accuracy**: The training loss and accuracy are relatively stable across epochs. However, the loss values are relatively high, which might indicate that the model is struggling to fit the data well.\n",
        "\n",
        "**Precision and Recall**: The precision and recall values are OK but not optimal. This model has higher recall values compared to precision, indicating it is better at identifying positive samples but at the cost of more false positives.\n",
        "\n",
        "**Validation Performance**: The validation performance shows fluctuating precision, recall, and F1 scores, indicating that the model might not generalise well on unseen data.\n",
        "\n",
        "**Potential Overfitting**: The fact that the model's training metrics are relatively stable while the validation metrics fluctuate might indicate overfitting."
      ],
      "metadata": {
        "id": "2E7eN-G3zIEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As there might be overfitting, trying with an increased value of L2=0.01\n",
        "\n"
      ],
      "metadata": {
        "id": "WHurXHyZuofb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Regularisation L2=0.01\")\n",
        "\n",
        "    # log model parameters\n",
        "    params = {\n",
        "        \"regularisation\": \"l2\",\n",
        "        \"l2_value\": 0.001,\n",
        "        \"input_dim\": X_train_tfidf_sparse.shape[1],\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # model training\n",
        "    history = model.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled\n",
        "    )\n",
        "\n",
        "    # log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # log model\n",
        "    mlflow.keras.log_model(model, \"model_regularisation_l2_0.01\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "    # end the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "9ZKXH4bBtHxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that with L2=0.01, there was not any significant learning across the epochs. This cuold now be due to underfitting.The consistent accuracy and low precision and recall on both training and validation sets suggest the model is not effectively learning the patterns in the data.\n",
        "\n",
        "We will have a look at trying L1 =0.001 and see what improvements there may be."
      ],
      "metadata": {
        "id": "k8YkfNYqDynD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2.2. L1\n",
        "\n",
        "We are also going to experiment with using L1 regularisation. L1 regularisation, unlike L2, encourages sparsity in the weight parameters, which can be beneficial in certain scenarios, especially if the model has a lot of input features, some of which might not be very informative. Given the sparse nature of the data we have decided to try this.\n",
        "\n",
        "### First for L1=0.001"
      ],
      "metadata": {
        "id": "0BousSQXqBBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Regularisation L1=0.001\")\n",
        "\n",
        "    # log model parameters\n",
        "    params = {\n",
        "        \"regularisation\": \"l1\",\n",
        "        \"l1_value\": 0.001,\n",
        "        \"input_dim\": X_train_tfidf_sparse.shape[1],\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # model training\n",
        "    history = model.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled\n",
        "    )\n",
        "\n",
        "    # log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # log model\n",
        "    mlflow.keras.log_model(model, \"model_regularisation_l1_0.001\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "    # end the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "LZTABiCbqDqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying L1=0.01"
      ],
      "metadata": {
        "id": "xI0WQa3AGbHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Regularisation L1=0.01\")\n",
        "\n",
        "    # log model parameters\n",
        "    params = {\n",
        "        \"regularisation\": \"l1\",\n",
        "        \"l1_value\": 0.01,\n",
        "        \"input_dim\": X_train_tfidf_sparse.shape[1],\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # model training\n",
        "    history = model.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled\n",
        "    )\n",
        "\n",
        "    # log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # log model\n",
        "    mlflow.keras.log_model(model, \"model_regularisation_l1_0.001\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "finally:\n",
        "    # end the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "ErsTbqmBGbmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating L1=0.01\n",
        "\n",
        "\n",
        "- **L1=0.01:**\n",
        "   - Loss: Slightly higher, indicating potential underfitting.\n",
        "   - Accuracy: Consistently around 0.7317, no learning progression observed.\n",
        "   - Precision and Recall: No significant change across epochs, suggesting the model is not effectively learning from the data.\n",
        "\n",
        "- **L1 =0.001:**\n",
        "   - Loss: Similar trend to l1=0.01, slightly lower.\n",
        "   - Accuracy: Constant at 0.73.., indicative of no learning.\n",
        "   - Precision and Recall: Again, no significant change, implying ineffective learning.\n",
        "\n",
        "- **L2 =0.01:**\n",
        "   - Loss: Generally higher than L1 models, suggesting underfitting.\n",
        "   - Accuracy: Static at 0.7317, indicating the model is not improving.\n",
        "   - Precision and Recall: unchanged, showing no learning progress.\n",
        "\n",
        "- **L2=0.001:**\n",
        "   - Loss: Lower compared to l2=0.01, but still shows signs of underfitting.\n",
        "   - Accuracy: stays at around 0.55-0.60, better than L1 but still not learning.\n",
        "   - Precision and Recall: Slightly better than L1 models but no significant learning or improvement.\n",
        "\n",
        "**Summary:**\n",
        "- Across all models, a static accuracy of around 0.7317 in L1 models and no significant learning progression in L2 models indicate that the regularisation may be too strong, causing underfitting.\n",
        "- Precision and recall metrics do not show significant improvement across different regularisation techniques and values.\n",
        "- Overall, increasing the regularisation strength (either L1 or L2) does not seem to benefit the model performance. In fact, it may be hindering the model's ability to learn from the data effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xy58IfGk4j88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.3 Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Ob-hyhe7U9c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3.1 Learning Rate\n",
        "\n",
        "\n",
        "Hyperparameter tuning is an important step in optimiaing machine learning models, and the learning rate is one of the most significant hyperparameters in training neural networks.\n",
        "\n",
        "The learning rate controls how much we adjust the weights of our network with respect to the loss gradient. It essentially dictates the step size during gradient descent. A properly tuned learning rate strikes a balance between two extremes. If the learning rate is too high, the model might converge too quickly to a suboptimal solution, or it might diverge, failing to find a solution. However, a very low learning rate makes the training process unnecessarily slow, as the model makes very tiny updates to the weights. It may also get stuck in local minima, missing the global minimum.\n",
        "\n",
        "Fine-tuning the learning rate can help the model converge faster to the optimal solution, saving time and computational resources. With the right learning rate, we can avoid overfitting and underfitting whcih has been seen so far.\n"
      ],
      "metadata": {
        "id": "uS8WwOSA4mFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining scheduler\n",
        "def scheduler(epoch, lr):\n",
        "\n",
        "    if epoch < 2:\n",
        "        return lr  # first 2 epochs orgional le\n",
        "    elif epoch < 4:\n",
        "        return lr * 0.5  # reduce every 2\n",
        "    elif epoch < 6:\n",
        "        return lr * 0.2  # again\n",
        "    elif epoch < 8:\n",
        "        return lr * 0.1  # again\n",
        "    else:\n",
        "        return lr * 0.05  # reduce to 5%\n",
        "\n",
        "# lr callback\n",
        "lr_scheduler = LearningRateScheduler(scheduler)\n",
        "initial_learning_rate = 0.001\n",
        "try:\n",
        "    # start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Hyperparameter Tuning - Learning Rate\")\n",
        "\n",
        "    # log model parameters\n",
        "    params = {\n",
        "        \"learning_rate_initial\": initial_learning_rate,\n",
        "        \"learning_rate_scheduler\": \"custom\",\n",
        "        \"input_dim\": X_train_tfidf_sparse.shape[1],\n",
        "        \"batch_size\": batch_size\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # model training with learning rate scheduler\n",
        "    history = model.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled,\n",
        "        callbacks=[lr_scheduler]\n",
        "    )\n",
        "\n",
        "    # log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # log model\n",
        "    mlflow.keras.log_model(model, \"model_learning_rate_tuning\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "APFlMWefVDDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the training set, both precision and recall are improving, leading to a higher F1 score. This indicates that the model is becoming better at correctly identifying positive samples and reducing false negatives and false positives.\n",
        "\n",
        "The validation metrics, especially the F1 score, are not improving as much as the training metrics. This could be due to overfitting. Both training and validation loss are decreasing, but the validation loss is not decreasing as much as the training loss, whgich could also be because of overfitting.\n",
        "\n",
        "\n",
        "## next time\n",
        "We are going to try:\n",
        "Early Stopping:  early stopping callback which monitors the validation loss and stops training when it stops improving.\n",
        "\n",
        "Simplify the Model: to mitigate overfitting. This will be done by reducing the number of layers or the number of units in each layer."
      ],
      "metadata": {
        "id": "Cmu6Gl2DFTiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Start an MLflow run\n",
        "    mlflow.start_run(run_name=\"With Early Stopping Model\")\n",
        "\n",
        "    # Log model parameters\n",
        "    params = {\n",
        "        \"num_layers\": 2,\n",
        "        \"num_neurons_layer1\": 64,\n",
        "        \"num_neurons_layer2\": 128,\n",
        "        \"num_neurons_output\": 6,\n",
        "        \"activation_layer1\": \"relu\",\n",
        "        \"activation_layer2\": \"relu\",\n",
        "        \"activation_output\": \"sigmoid\",\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"loss_function\": \"binary_crossentropy\",\n",
        "        \"batch_size\": batch_size,\n",
        "        \"regularization_l2\": 0.001\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # Define the model\n",
        "    model_use_1 = Sequential()\n",
        "    model_use_1.add(Dense(64, activation='relu', input_dim=X_train_tfidf_sparse.shape[1], kernel_regularizer=l2(0.001)))\n",
        "    model_use_1.add(Dropout(0.5))\n",
        "    model_use_1.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model_use_1.add(Dropout(0.5))\n",
        "    model_use_1.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    model_use_1.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf_metrics.Precision(name='precision'),\n",
        "                           tf_metrics.Recall(name='recall'),\n",
        "                           tfa.metrics.F1Score(num_classes=6, average='macro', name='f1_score')])\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    # Train the model\n",
        "    history_resampled = model_use_1.fit(\n",
        "        train_gen_resampled,\n",
        "        epochs=10,\n",
        "        validation_data=val_gen_resampled,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    # Log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history_resampled.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history_resampled.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history_resampled.history['precision'][-1],\n",
        "        \"validation_precision\": history_resampled.history['val_precision'][-1],\n",
        "        \"train_recall\": history_resampled.history['recall'][-1],\n",
        "        \"validation_recall\": history_resampled.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history_resampled.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history_resampled.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.keras.log_model(model_use_1, \"simplified_model\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "XYtBtt2_KFWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy and Loss**: The training accuracy increases, and the loss decreases over epochs, which is a good sign. However, the validation accuracy and loss have some fluctuations, suggesting that the model might still be struggling to generalise well to unseen data.\n",
        "\n",
        "**Precision and Recall**: The precision and recall on the training data are quite high, this shows the model is performing well on the data it has seen. But the lower precision and recall on the validation data also mean that the model is not generalising very well.\n",
        "\n",
        "**Overfitting**: The gap between training and validation metrics could be a sign of overfitting. The model is performing significantly better on the training data compared to the validation data.\n",
        "\n",
        "**Early Stopping**: The early stopping can be seen to be working correctly. It's preventing the model from continuing to train once it's clear that further epochs are not leading to better performance on the validation set.\n",
        "\n",
        "**Best Learning Rate**: Based on the above reuslts the best learning rate is Epoch 3 where lr=0.0005. This is where we can see improvements in both training and validation metrics, however there are indications of overfitting which we will keep an eye on in the next time.\n",
        "\n",
        "**Next Steps**: We will try hyperparamter tuning the number of neurons and the number of layers in the model."
      ],
      "metadata": {
        "id": "kXAkSOd1wcCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning neurons and layers and using cross validation\n"
      ],
      "metadata": {
        "id": "8_MQT5mvw1pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Hyperparameter Tuning - Neurons and Layers\")\n",
        "\n",
        "    # Wrap the model using KerasClassifier\n",
        "    model_tuning = KerasClassifier(model=create_model, epochs=10, verbose=1)\n",
        "\n",
        "    # Parameter grid for grid search\n",
        "    param_grid = {\n",
        "        'model__n_layers': [1, 2, 3],\n",
        "        'model__n_neurons': [64, 128, 256]\n",
        "    }\n",
        "\n",
        "    # Initialize GridSearchCV\n",
        "    grid = GridSearchCV(estimator=model_tuning, param_grid=param_grid, cv=3, verbose=3, n_jobs=-1)\n",
        "\n",
        "    # Convert to dense format as required\n",
        "    X_train_dense = X_train_tfidf_sparse.toarray()\n",
        "\n",
        "    # Perform grid search\n",
        "    grid_result = grid.fit(X_train_dense, y_train)\n",
        "\n",
        "    # Log the results\n",
        "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "    mlflow.log_metric(\"best_score\", grid_result.best_score_)\n",
        "    mlflow.log_params(grid_result.best_params_)\n",
        "\n",
        "    # Log the parameter grid used for tuning\n",
        "    mlflow.log_dict(param_grid, \"param_grid.json\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "qYqms-MV5Vc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first approach we used a sequential model with early stopping and a learning rate scheduler. This setup included two hidden layers (64 and 128 neurons) and L2 regularisation applied to each layer.In the grid search approach, we explored different combinations of layers and neurons.\n",
        "\n",
        "\n",
        "The first model's training showed a gradual improvement in precision and recall, suggesting that the learning rate scheduler and early stopping were effectively fine-tuning the model. The grid search results appear to show better training accuracy but lower precision and recall values. This might indicate that while the model is good at classifying the majority class but it struggles with the minority class which is a common issue in imbalanced datasets.\n",
        "\n",
        "### Looking in more detail at each model:"
      ],
      "metadata": {
        "id": "03Gm0I3juayp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# again need to convert to desne\n",
        "X_test_dense = X_test_tfidf_sparse.toarray()\n",
        "\n",
        "# Evaluate model_use_1 on test data\n",
        "test_loss, test_accuracy, test_precision, test_recall, test_f1_score = model_use_1.evaluate(X_test_dense, y_test, verbose=0)\n",
        "\n",
        "\n",
        "print(\"First Model Performance:\")\n",
        "print(f\"Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1-Score: {test_f1_score}\")\n"
      ],
      "metadata": {
        "id": "UkkTLX8Nu4TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**: 43.10% - This isnt very good but given the multi-label nature of the problem, this might not be the most informative metric.\n",
        "\n",
        "**Precision**: 18.12% - This is relatively low, indicating that a significant portion of the positive identifications made by the model were incorrect.\n",
        "\n",
        "**Recall**: 64.31% - This suggests that the model is relatively better at identifying all relevant instances in the test dataset. A higher recall indicates fewer false negatives.\n",
        "\n",
        "**F1-Score**: 6.73% - The F1-Score is quite low, which is not surprising given the low precision. The F1-Score indicates poor overall performance."
      ],
      "metadata": {
        "id": "FuJdIF3ccSLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Start an MLflow run\n",
        "    mlflow.start_run(run_name=\"Model Training - Model 2\")\n",
        "\n",
        "    # Define model parameters\n",
        "    params = {\n",
        "        \"num_layers\": 2,\n",
        "        \"num_neurons_per_layer\": 128,\n",
        "        \"regularizer\": \"l2\",\n",
        "        \"regularizer_value\": 0.001,\n",
        "        \"dropout_rate\": 0.5,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"input_dim\": X_train_tfidf_sparse.shape[1]\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    # Define model architecture\n",
        "    model_2 = Sequential()\n",
        "    model_2.add(Dense(128, activation='relu', input_dim=X_train_tfidf_sparse.shape[1], kernel_regularizer=l2(0.001)))\n",
        "    model_2.add(Dropout(0.5))\n",
        "    model_2.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "    model_2.add(Dropout(0.5))\n",
        "    model_2.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model_2.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy', tf_metrics.Precision(name='precision'),\n",
        "                             tf_metrics.Recall(name='recall'),\n",
        "                             tfa.metrics.F1Score(num_classes=6, average='macro', name='f1_score')])\n",
        "\n",
        "    # Train the model\n",
        "    history = model_2.fit(X_train_dense, y_train, epochs=10, verbose=1)\n",
        "\n",
        "    # Log metrics\n",
        "    metrics = {\n",
        "        \"train_accuracy\": history.history['accuracy'][-1],\n",
        "        \"validation_accuracy\": history.history['val_accuracy'][-1],\n",
        "        \"train_precision\": history.history['precision'][-1],\n",
        "        \"validation_precision\": history.history['val_precision'][-1],\n",
        "        \"train_recall\": history.history['recall'][-1],\n",
        "        \"validation_recall\": history.history['val_recall'][-1],\n",
        "        \"train_f1_score\": history.history['f1_score'][-1],\n",
        "        \"validation_f1_score\": history.history['val_f1_score'][-1]\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "\n",
        "    # Log the model\n",
        "    mlflow.keras.log_model(model_2, \"model_2\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "finally:\n",
        "    # End the MLflow run\n",
        "    mlflow.end_run()\n"
      ],
      "metadata": {
        "id": "hWCv8jxkxsL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model_2\n",
        "test_loss_2, test_accuracy_2, test_precision_2, test_recall_2, test_f1_score_2 = model_2.evaluate(X_test_dense, y_test, verbose=0)\n",
        "print(\"Second Model Performance:\")\n",
        "print(f\"Accuracy: {test_accuracy_2}, Precision: {test_precision_2}, Recall: {test_recall_2}, F1-Score: {test_f1_score_2}\")"
      ],
      "metadata": {
        "id": "pJ4vmJ6PeLNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting metrics to compare\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "first_model_results = [test_accuracy, test_precision, test_recall, test_f1_score]\n",
        "second_model_results = [test_accuracy_2, test_precision_2, test_recall_2, test_f1_score_2]\n",
        "\n",
        "x = range(len(metrics))\n",
        "plt.bar(x, first_model_results, width=0.4, label='First Model', align='center')\n",
        "plt.bar(x, second_model_results, width=0.4, label='Second Model', align='edge')\n",
        "plt.xticks(x, metrics)\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9dhxXeiTcju_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Model Performance**:\n",
        "- Accuracy: 43.10%\n",
        "- Precision: 18.12%\n",
        "- Recall: 64.31%\n",
        "- F1-Score: 6.73%\n",
        "\n",
        "**Second Model Performance**:\n",
        "- Accuracy: 99.76%\n",
        "- Precision: 67.53%\n",
        "- Recall: 37.03%\n",
        "- F1-Score: 2.90%\n",
        "\n",
        "**Conclusions and Comparisons**:\n",
        "**Accuracy**: The second model has a significantly higher accuracy compared to the first model. This suggests that the second model is much better at correctly classifying the majority of instances in the test data.\n",
        "\n",
        "**Precision and Recall**: While the second model has a higher precision than the first model, its recall is significantly lower. This indicates that the second model, while more precise in its positive predictions, fails to identify a substantial portion of the actual positive cases. The first model, despite its lower precision, is better at identifying a larger proportion of the actual positive cases.\n",
        "\n",
        "**F1-Score**: The F1-score is a balance between precision and recall, and it's particularly useful when there is an uneven class distribution. The first model has a higher F1-score, suggesting a better balance between precision and recall compared to the second model.\n",
        "\n",
        "**Overfitting Concern**: The extremely high accuracy of the second model (nearly 100%) compared to its performance in precision, recall, and F1-score suggests potential overfitting. This means the model may be too closely tailored to the training data, reducing its ability to generalize to new, unseen data.\n",
        "\n",
        "**Model Complexity and Data Representation**: The disparity in performance could also be attributed to the difference in model complexity (number of layers and neurons) and the way the data was fed into the models (dense vs. sparse format).\n",
        "\n",
        "**Recommendations for Improvement in future Experiments**:\n",
        "- **Model Complexity**: Experiment with different architectures to find a more optimal balance between model complexity and performance.\n",
        "- **Data Preparation**: Ensure consistent data preparation steps for both models. If sparse formats were used successfully in the first model, consider applying similar techniques to the second model.\n",
        "- **regularisation and Dropout**: To prevent overfitting, especially for the second model, we could try to adjust regularisation parameters or dropout rates.\n",
        "- **Model Evaluation Metrics**: Depending on the specific use case, consider which metrics are most crucial (e.g., is minimizing false positives more important than capturing as many true positives as possible? Also need to consider user impact) and adjust the model accordingly."
      ],
      "metadata": {
        "id": "1IDSwaSyg0HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliography\n",
        "[1] Chollet\n",
        "\n",
        "[2] [Word2Vec](https://www.tensorflow.org/text/tutorials/word2vec )"
      ],
      "metadata": {
        "id": "cTn3Y7g_V8rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "from nbconvert import HTMLExporter\n",
        "\n",
        "# Load the notebook (.ipynb file)\n",
        "notebook_path = \"/content/drive/MyDrive/Colab Notebooks/Final_NN_CW1.ipynb\"\n",
        "with open(notebook_path, 'r', encoding='utf-8') as notebook_file:\n",
        "    notebook_content = nbformat.read(notebook_file, as_version=4)\n",
        "\n",
        "# Convert to HTML\n",
        "html_exporter = HTMLExporter()\n",
        "(html_output, resources) = html_exporter.from_notebook_node(notebook_content)\n",
        "\n",
        "# Save the HTML output to a file\n",
        "output_path = \"/content/drive/MyDrive/MSc/NeuralNetworks/Final_NN_CW1.html\"\n",
        "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "    output_file.write(html_output)\n"
      ],
      "metadata": {
        "id": "KVcCEEfXOwC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "em0ND_GMPBDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}